We are trying to predict whether or not a given customer was satisfied with their order based on their answers to six questions. The customers gave a numerical score based on how much they agreed with each statement and indicated whether or not they were satisfied. By building some sort of binary classification model, we can not only try to predict whether or not customers are satisfied, but also discover what is most important to customers and thus what the company can focus on for improvement.  There were no missing values in the dataset and the data was entirely numerical – consisting of integer values in a single range. Furthermore, the dataset was reasonably well balanced (slight imbalance in favour of happy customers). This meant that no preprocessing  was required, save for creating labels. 

My first thought was to build a Logistic Regression model. However, although the variance inflation factor was low for all variables, there did not appear to be a linear relationship between the independent variables and the logit of the outcomes and as such I had to rule out logistic regression. Furthermore, I ruled out neural networks due to the small size of the dataset which would likely lead to overfitting. Extreme gradient boosted tree models are very suitable for tabular data and have been shown to generally have high accuracy on such data. Given the nature of the problem we will be using a classification tree model with a  binary logistic objective.  We employ bayesian optimisation to tune the hyper-parameters and we use K-fold cross fold validation to check for overfitting. Features are systematically removed to study their relative importance. Models are judged predominantly based on accuracy. Confusion matrices and ROC will also be employed to study model performance.

The file titled Apziva_Customer_Dataset_XGB trains and validates a model with an 80/20 split and returns values for training and test accuracy, as well as a confusion matrix and ROC. It has an accuracy across the test set of 73.1% and a training accuracy of 80%. This indicates (but does not guarantee) that the model does not overfit the data. The confusion matrix indicates high sensitivity, but lower specificity. This indicates that the slight data imbalance is more of an issue than we first thought. One way to counter this problem is by tuning the probability threshold (base score in XGBoost). However changing the probability threshold did not improve performance. 

The file titled Apziva_Customer_Dataset_XGB_Bayes runs bayesian optimisation on the model (optimising to maximise average accuracy across 5-fold cross validation). The script runs the optimisation across the following hyper-parameters: max depth, learning rate, base score, number of estimators, L1 regularisation, and L2 regularisation. 

The file titled Apziva_Customer_Dataset_XGB_Cross_Fold_Optimised trains and validates a model with 5-fold cross validation. It is worth noting that this model has different parameter values than the model from the first file and so there is a choice of two models.

Removing the question “my order was delivered on time” resulted in a major reduction in accuracy. This is perhaps unsurprising as tardiness of delivery is likely to cause dissatisfaction and as such there will likely be a pattern relating the answer to this question and the outcome. This is a key area for the logistics company who conducted the survey to focus on, but it may prove somewhat difficult as it depends on traffic, weather, customs etc. The removal of the question “I ordered everything I wanted to order” actually led to an increase in accuracy across the 5-fold cross validation. As such this may be a question which could potentially be removed from future surveys. Removing questions X2, X4, and, X6 led to no change in average cross fold accuracy and  the combined removal of X2, X3, X4, and X6 led to improved accuracy. This means that questions X1 (“my order was delivered on time”) and X5 (“I am satisfied with my courier”) preserved accuracy by themselves. However given the relatively small size of the dataset, these results should be taken with caution. 

   
 
